<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Mocap-dense-trajectories : Dense trajectories generated from mocap data.">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Mocap-dense-trajectories</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/jltmtz/mocap-dense-trajectories">View on GitHub</a>

          <h1 id="project_title">Mocap-dense-trajectories</h1>
          <h2 id="project_tagline">Dense trajectories generated from mocap data.</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/jltmtz/mocap-dense-trajectories/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/jltmtz/mocap-dense-trajectories/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
<a name="mocap-dense-trajectories" class="anchor" href="#mocap-dense-trajectories"><span class="octicon octicon-link"></span></a>mocap-dense-trajectories</h1>

<p><img src="http://www.cs.ubc.ca/%7Ejulm/imgs/trajectory_generation.png" alt="The Process in a Nuthsell"></p>

<p>This code generates dense trajectories similar to <a href="https://lear.inrialpes.fr/people/wang/dense_trajectories">those of Wang et. al</a>, [1]
but generated from mocap data, instead of video sequences. For an extended 
description visit our <a href="http://jltmtz.github.io/mocap-dense-trajectories/">project website</a>.</p>

<p>This code was written mainly by <a href="http://www.cs.ubc.ca/%7Eankgupta/">Ankur Gupta</a> and <a href="http://www.cs.ubc.ca/%7Ejulm/">Julieta Martinez</a>.</p>

<h2>
<a name="usage" class="anchor" href="#usage"><span class="octicon octicon-link"></span></a>Usage</h2>

<p>The input is a .bvh file. You can find the entire CMU mocap dataset converted to bvh format <a href="https://sites.google.com/a/cgspeed.com/cgspeed/motion-capture/cmu-bvh-conversion">on the internet</a>.</p>

<p>To generate trajectories from a sample file, run <code>demo_trajectory_generation</code>. To see nice visualizations of the process and some of the nuts and bolts of how this is done, run <code>demo_trajectory_generation_2</code>.</p>

<p>The main function that you want to call is <code>imocap2trajectories</code>. The output is an n-by-(7 + trajectory_length*2) matrix where each row has the following entries:</p>

<pre><code>frameNum:     The trajectory ends on this frame
mean_x:       The mean value of the x coordinates of the trajectory
mean_y:       The mean value of the y coordinates of the trajectory
var_x:        The variance of the x coordinates of the trajectory
var_y:        The variance of the y coordinates of the trajectory
length:       The length of the trajectory
scale:        This information is lost due to ortographic projection. Set to -1.
Trajectory:   2x[trajectory length] (default 30 dimension). x and y entries of the trajectory.
</code></pre>

<p>As opposed to the video dense trajectories, we obviously do not compute visual descriptors along the trajectories.</p>

<h2>
<a name="citation" class="anchor" href="#citation"><span class="octicon octicon-link"></span></a>Citation</h2>

<p>If you use this code, please cite our CVPR 14 paper:</p>

<pre><code>A. Gupta, J. Martinez, J. J. Little and R. J. Woodham. "Pose from Motion for Cross-view Action Recognition via Non-linear Circulant Temporal Encoding". In CVPR, 2014.
</code></pre>

<p>Bibtex:</p>

<pre><code>@inproceedings{gupta20143dpose,
  title={{3D Pose from Motion for Cross-view Action Recognition via Non-linear Circulant Temporal Encoding}},
  author={Gupta, Ankur and Martinez, Julieta and Little, James J. and Woodham, Robert J.},
  booktitle={CVPR},
  year={2014}
}
</code></pre>

<h2>
<a name="references" class="anchor" href="#references"><span class="octicon octicon-link"></span></a>References</h2>

<ol>
<li>Wang, Heng, et al. "Action recognition by dense trajectories." Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, 2011.</li>
<li>Katz, Sagi, Ayellet Tal, and Ronen Basri. "Direct visibility of point sets." ACM Transactions on Graphics (TOG). Vol. 26. No. 3. ACM, 2007.</li>
</ol><h2>
<a name="acknowledgements" class="anchor" href="#acknowledgements"><span class="octicon octicon-link"></span></a>Acknowledgements</h2>

<p>We include the following third-party code for user's convenience. We Thank the original authors for making their code publicly available:</p>

<ul>
<li>bvh-matlab by Will Robertson. Complete project accessible <a href="https://github.com/wspr/bvh-matlab">here</a>.</li>
<li>Hidden Point Removal by Sagi Katz. Hosted at <a href="http://www.mathworks.com/matlabcentral/fileexchange/16581-hidden-point-removal">Matlab Central</a>.</li>
</ul>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Mocap-dense-trajectories maintained by <a href="https://github.com/jltmtz">jltmtz</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
