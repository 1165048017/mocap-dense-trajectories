{"name":"Mocap-dense-trajectories","tagline":"Dense trajectories generated from mocap data.","body":"mocap-dense-trajectories\r\n========================\r\n\r\nThis code generates dense trajectories similar to [those of Wang et. al](https://lear.inrialpes.fr/people/wang/dense_trajectories), [1]\r\nbut generated from mocap data, instead of video sequences. For an extended \r\ndescription visit our [project website](http://jltmtz.github.io/mocap-dense-trajectories/).\r\n\r\nThis code was written mainly by [Ankur Gupta](http://www.cs.ubc.ca/~ankgupta/) and [Julieta Martinez](http://www.cs.ubc.ca/~julm/).\r\n\r\n![The Process in a Nuthsell](http://www.cs.ubc.ca/~julm/imgs/trajectory_generation.png)\r\n\r\nUsage\r\n-----\r\nThe input is a .bvh file. You can find the entire CMU mocap dataset converted to bvh format [on the internet](https://sites.google.com/a/cgspeed.com/cgspeed/motion-capture/cmu-bvh-conversion).\r\n\r\nTo generate trajectories from a sample file, run `demo_trajectory_generation`. To see nice visualizations of the process and some of the nuts and bolts of how this is done, run `demo_trajectory_generation_2`.\r\n\r\nThe main function that you want to call is `imocap2trajectories`. The output is an n-by-(7 + trajectory_length*2) matrix where each row has the following entries:\r\n\r\n```\r\nframeNum:     The trajectory ends on this frame\r\nmean_x:       The mean value of the x coordinates of the trajectory\r\nmean_y:       The mean value of the y coordinates of the trajectory\r\nvar_x:        The variance of the x coordinates of the trajectory\r\nvar_y:        The variance of the y coordinates of the trajectory\r\nlength:       The length of the trajectory\r\nscale:        This information is lost due to ortographic projection. Set to -1.\r\nTrajectory:   2x[trajectory length] (default 30 dimension). x and y entries of the trajectory.\r\n```\r\n\r\nAs opposed to the video dense trajectories, we obviously do not compute visual descriptors along the trajectories.\r\n\r\nCitation\r\n--------\r\nIf you use this code, please cite our CVPR 14 paper:\r\n\r\n```\r\nA. Gupta, J. Martinez, J. J. Little and R. J. Woodham. \"Pose from Motion for Cross-view Action Recognition via Non-linear Circulant Temporal Encoding\". In CVPR, 2014.\r\n```\r\n\r\nBibtex:\r\n```\r\n@inproceedings{gupta20143dpose,\r\n  title={{3D Pose from Motion for Cross-view Action Recognition via Non-linear Circulant Temporal Encoding}},\r\n  author={Gupta, Ankur and Martinez, Julieta and Little, James J. and Woodham, Robert J.},\r\n  booktitle={CVPR},\r\n  year={2014}\r\n}\r\n```\r\n\r\nReferences\r\n----------\r\n\r\n1. Wang, Heng, et al. \"Action recognition by dense trajectories.\" Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, 2011.\r\n2. Katz, Sagi, Ayellet Tal, and Ronen Basri. \"Direct visibility of point sets.\" ACM Transactions on Graphics (TOG). Vol. 26. No. 3. ACM, 2007.\r\n\r\n\r\nAcknowledgements\r\n----------\r\nWe include the following third-party code for user's convenience. We Thank the original authors for making their code publicly available:\r\n- bvh-matlab by Will Robertson. Complete project accessible [here](https://github.com/wspr/bvh-matlab).\r\n- Hidden Point Removal by Sagi Katz. Hosted at [Matlab Central](http://www.mathworks.com/matlabcentral/fileexchange/16581-hidden-point-removal).\r\n \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}